<feed xmlns="http://www.w3.org/2005/Atom">
  <title>The Odd Bit</title>

  
  <link rel="self" href="https://blog.oddbit.com/tag/openstack/atom.xml"/>
  
  <link href="https://blog.oddbit.com/tag/openstack/" rel="alternate"></link>

  <updated>2019-12-19T00:00:00Z</updated>
  <id>https://blog.oddbit.com/tag/openstack/</id>
  <entry>
    <title>OVN and DHCP: A minimal example</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/"/>
    <id>https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/</id>
    <published>2019-12-19T00:00:00Z</published>
    <updated>2019-12-19T00:00:00Z</updated>
    <summary type="html">Introduction A long time ago, I wrote an article all about OpenStack Neutron (which at that time was called Quantum). That served as an excellent reference for a number of years, but if you&amp;rsquo;ve deployed a recent version of OpenStack you may have noticed that the network architecture looks completely different. The network namespaces previously used to implement routers and dhcp servers are gone (along with iptables rules and other features), and have been replaced by OVN (&amp;ldquo;Open Virtual Network&amp;rdquo;).</summary>
  </entry>
  
  <entry>
    <title>Running Keystone with Docker Compose</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/"/>
    <id>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</id>
    <published>2019-06-07T00:00:00Z</published>
    <updated>2019-06-07T00:00:00Z</updated>
    <summary type="html">In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s Keystone service (and the requisite database server) in containers using Docker Compose.
Running MariaDB The standard mariadb docker image can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple docker command line for starting MariaDB might look something like:</summary>
  </entry>
  
  <entry>
    <title>Grouping aggregation queries in Gnocchi 4.0.x</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2018-02-26-grouping-aggregation-queries-i/"/>
    <id>https://blog.oddbit.com/post/2018-02-26-grouping-aggregation-queries-i/</id>
    <published>2018-02-26T00:00:00Z</published>
    <updated>2018-02-26T00:00:00Z</updated>
    <summary type="html">In this article, we&amp;rsquo;re going to ask Gnocchi (the OpenStack telemetry storage service) how much memory was used, on average, over the course of each day by each project in an OpenStack environment.
Environment I&amp;rsquo;m working with an OpenStack &amp;ldquo;Pike&amp;rdquo; deployment, which means I have Gnocchi 4.0.x. More recent versions of Gnocchi (4.1.x and later) have a new aggregation API called dynamic aggregates, but that isn&amp;rsquo;t available in 4.0.x so in this article we&amp;rsquo;ll be using the legacy /v1/aggregations API.</summary>
  </entry>
  
  <entry>
    <title>Safely restarting an OpenStack server with Ansible</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2018-01-24-safely-restarting-an-openstack-server-wi/"/>
    <id>https://blog.oddbit.com/post/2018-01-24-safely-restarting-an-openstack-server-wi/</id>
    <published>2018-01-24T00:00:00Z</published>
    <updated>2018-01-24T00:00:00Z</updated>
    <summary type="html">The other day on #ansible, someone was looking for a way to safely shut down a Nova server, wait for it to stop, and then start it up again using the openstack cli. The first part seemed easy:
- hosts: myserver tasks: - name: shut down the server command: poweroff become: true  &amp;hellip;but that will actually fail with the following result:
TASK [shut down server] ************************************* fatal: [myserver]: UNREACHABLE! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;msg&amp;quot;: &amp;quot;Failed to connect to the host via ssh: Shared connection to 10.</summary>
  </entry>
  
  <entry>
    <title>Ansible for Infrastructure Testing</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2017-08-02-ansible-for-infrastructure-testing/"/>
    <id>https://blog.oddbit.com/post/2017-08-02-ansible-for-infrastructure-testing/</id>
    <published>2017-08-02T00:00:00Z</published>
    <updated>2017-08-02T00:00:00Z</updated>
    <summary type="html">At $JOB we often find ourselves at customer sites where we see the same set of basic problems that we have previously encountered elsewhere (&amp;ldquo;your clocks aren&amp;rsquo;t in sync&amp;rdquo; or &amp;ldquo;your filesystem is full&amp;rdquo; or &amp;ldquo;you haven&amp;rsquo;t installed a critical update&amp;rdquo;, etc). We would like a simple tool that could be run either by the customer or by our own engineers to test for and report on these common issues. Fundamentally, we want something that acts like a typical code test suite, but for infrastructure.</summary>
  </entry>
  
  <entry>
    <title>OpenStack, Containers, and Logging</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2017-06-14-openstack-containers-and-logging/"/>
    <id>https://blog.oddbit.com/post/2017-06-14-openstack-containers-and-logging/</id>
    <published>2017-06-14T00:00:00Z</published>
    <updated>2017-06-14T00:00:00Z</updated>
    <summary type="html">I&amp;rsquo;ve been thinking about logging in the context of OpenStack and containerized service deployments. I&amp;rsquo;d like to lay out some of my thoughts on this topic and see if people think I am talking crazy or not.
There are effectively three different mechanisms that an application can use to emit log messages:
 Via some logging-specific API, such as the legacy syslog API By writing a byte stream to stdout/stderr By writing a byte stream to a file  A substantial advantage to the first mechanism (using a logging API) is that the application is logging messages rather than bytes.</summary>
  </entry>
  
  <entry>
    <title>Making sure your Gerrit changes aren&#39;t broken</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2017-01-22-making-sure-your-gerrit-changes-aren-t-b/"/>
    <id>https://blog.oddbit.com/post/2017-01-22-making-sure-your-gerrit-changes-aren-t-b/</id>
    <published>2017-01-22T00:00:00Z</published>
    <updated>2017-01-22T00:00:00Z</updated>
    <summary type="html">It&amp;rsquo;s a bit of an embarrassment when you submit a review to Gerrit only to have it fail CI checks immediately because of something as simple as a syntax error or pep8 failure that you should have caught yourself before submitting&amp;hellip;but you forgot to run your validations before submitting the change.
In many cases you can alleviate this through the use of the git pre-commit hook, which will run every time you commit changes locally.</summary>
  </entry>
  
  <entry>
    <title>Exploring YAQL Expressions</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2016-08-11-exploring-yaql-expressions/"/>
    <id>https://blog.oddbit.com/post/2016-08-11-exploring-yaql-expressions/</id>
    <published>2016-08-11T00:00:00Z</published>
    <updated>2016-08-11T00:00:00Z</updated>
    <summary type="html">The Newton release of Heat adds support for a yaql intrinsic function, which allows you to evaluate yaql expressions in your Heat templates. Unfortunately, the existing yaql documentation is somewhat limited, and does not offer examples of many of yaql&amp;rsquo;s more advanced features.
I am working on a Fluentd composable service for TripleO. I want to allow each service to specify a logging source configuration fragment, for example:
parameters: NovaAPILoggingSource: type: json description: Fluentd logging configuration for nova-api.</summary>
  </entry>
  
  <entry>
    <title>Connecting another vm to your tripleo-quickstart deployment</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2016-05-19-connecting-another-vm-to-your-tripleo-qu/"/>
    <id>https://blog.oddbit.com/post/2016-05-19-connecting-another-vm-to-your-tripleo-qu/</id>
    <published>2016-05-19T00:00:00Z</published>
    <updated>2016-05-19T00:00:00Z</updated>
    <summary type="html">Let&amp;rsquo;s say that you have set up an environment using tripleo-quickstart and you would like to add another virtual machine to the mix that has both &amp;ldquo;external&amp;rdquo; connectivity (&amp;ldquo;external&amp;rdquo; in quotes because I am using it in the same way as the quickstart does w/r/t the undercloud) and connectivity to the overcloud nodes. How would you go about setting that up?
For a concrete example, let&amp;rsquo;s presume you have deployed an environment using the default tripleo-quickstart configuration, which looks like this:</summary>
  </entry>
  
  <entry>
    <title>Deploying an HA OpenStack development environment with tripleo-quickstart
</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-development-envir/"/>
    <id>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-development-envir/</id>
    <published>2016-02-19T00:00:00Z</published>
    <updated>2016-02-19T00:00:00Z</updated>
    <summary type="html">In this article I would like to introduce tripleo-quickstart, a tool that will automatically provision a virtual environment and then use TripleO to deploy an HA OpenStack on top of it.
Introducing Tripleo-Quickstart The goal of the Tripleo-Quickstart project is to replace the instack-virt-setup tool for quickly setting up virtual TripleO environments, and to ultimately become the tool used by both developers and upstream CI for this purpose. The project is a set of Ansible playbooks that will take care of:</summary>
  </entry>
  
  <entry>
    <title>Ansible 2.0: New OpenStack modules</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-10-26-ansible-20-new-openstack-modules/"/>
    <id>https://blog.oddbit.com/post/2015-10-26-ansible-20-new-openstack-modules/</id>
    <published>2015-10-26T00:00:00Z</published>
    <updated>2015-10-26T00:00:00Z</updated>
    <summary type="html">This is the second in a loose sequence of articles looking at new features in Ansible 2.0. In the previous article I looked at the Docker connection driver. In this article, I would like to provide an overview of the new-and-much-improved suite of modules for interacting with an OpenStack environment, and provide a few examples of their use.
In versions of Ansible prior to 2.0, there was a small collection of OpenStack modules.</summary>
  </entry>
  
  <entry>
    <title>Migrating Cinder volumes between OpenStack environments using shared NFS storage</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-09-29-migrating-cinder-volumes-between-openstack-environments/"/>
    <id>https://blog.oddbit.com/post/2015-09-29-migrating-cinder-volumes-between-openstack-environments/</id>
    <published>2015-09-29T00:00:00Z</published>
    <updated>2015-09-29T00:00:00Z</updated>
    <summary type="html">Many of the upgrade guides for OpenStack focus on in-place upgrades to your OpenStack environment. Some organizations may opt for a less risky (but more hardware intensive) option of setting up a parallel environment, and then migrating data into the new environment. In this article, we look at how to use Cinder backups with a shared NFS volume to facilitate the migration of Cinder volumes between two different OpenStack environments.</summary>
  </entry>
  
  <entry>
    <title>Provider external networks (in an appropriate amount of detail)</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-08-13-provider-external-networks-details/"/>
    <id>https://blog.oddbit.com/post/2015-08-13-provider-external-networks-details/</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html">In Quantum in Too Much Detail, I discussed the architecture of a Neutron deployment in detail. Since that article was published, Neutron gained the ability to handle multiple external networks with a single L3 agent. While I wrote about that back in 2014, I covered the configuration side of it in much more detail than I discussed the underlying network architecture. This post addresses the architecture side.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:</summary>
  </entry>
  
  <entry>
    <title>In which we are amazed it doesn&#39;t all fall apart</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-07-26-in-which-we-are-amazed-it-doesnt-all-fall-apart/"/>
    <id>https://blog.oddbit.com/post/2015-07-26-in-which-we-are-amazed-it-doesnt-all-fall-apart/</id>
    <published>2015-07-26T00:00:00Z</published>
    <updated>2015-07-26T00:00:00Z</updated>
    <summary type="html">So, the Kilo release notes say:
nova-manage migrate-flavor-data  But nova-manage says:
nova-manage db migrate_flavor_data  But that says:
Missing arguments: max_number  And the help says:
usage: nova-manage db migrate_flavor_data [-h] [--max-number &amp;lt;number&amp;gt;]  Which indicates that &amp;ndash;max-number is optional, but whatever, so you try:
nova-manage db migrate_flavor_data --max-number 100  And that says:
Missing arguments: max_number  So just for kicks you try:
nova-manage db migrate_flavor_data --max_number 100  And that says:</summary>
  </entry>
  
  <entry>
    <title>OpenStack Networking without DHCP</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-06-26-openstack-networking-without-dhcp/"/>
    <id>https://blog.oddbit.com/post/2015-06-26-openstack-networking-without-dhcp/</id>
    <published>2015-06-26T00:00:00Z</published>
    <updated>2015-06-26T00:00:00Z</updated>
    <summary type="html">In an OpenStack environment, cloud-init generally fetches information from the metadata service provided by Nova. It also has support for reading this information from a configuration drive, which under OpenStack means a virtual CD-ROM device attached to your instance containing the same information that would normally be available via the metadata service.
It is possible to generate your network configuration from this configuration drive, rather than relying on the DHCP server provided by your OpenStack environment.</summary>
  </entry>
  
</feed>
