<feed xmlns="http://www.w3.org/2005/Atom">
  <title>The Odd Bit</title>

  
  <link rel="self" href="https://blog.oddbit.com/tag/kubernetes/atom.xml"/>
  
  <link href="https://blog.oddbit.com/tag/kubernetes/" rel="alternate"></link>

  <updated>2021-09-03T00:00:00Z</updated>
  <id>https://blog.oddbit.com/tag/kubernetes/</id>
  <entry>
    <title>Kubernetes External Secrets</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2021-09-03-kubernetes-external-secrets/"/>
    <id>https://blog.oddbit.com/post/2021-09-03-kubernetes-external-secrets/</id>
    <published>2021-09-03T00:00:00Z</published>
    <updated>2021-09-03T00:00:00Z</updated>
    <summary type="html">At $JOB we maintain the configuration for our OpenShift clusters in a public git repository. Changes in the git repository are applied automatically using ArgoCD and Kustomize. This works great, but the public nature of the repository means we need to find a secure solution for managing secrets (such as passwords and other credentials necessary for authenticating to external services). In particular, we need a solution that permits our public repository to be the source of truth for our cluster configuration, without compromising our credentials.</summary>
  </entry>
  
  <entry>
    <title>Connecting OpenShift to an External Ceph Cluster</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2021-08-23-external-ocs/"/>
    <id>https://blog.oddbit.com/post/2021-08-23-external-ocs/</id>
    <published>2021-08-23T00:00:00Z</published>
    <updated>2021-08-23T00:00:00Z</updated>
    <summary type="html">Red Hat&amp;rsquo;s OpenShift Data Foundation (formerly &amp;ldquo;OpenShift Container Storage&amp;rdquo;, or &amp;ldquo;OCS&amp;rdquo;) allows you to either (a) automatically set up a Ceph cluster as an application running on your OpenShift cluster, or (b) connect your OpenShift cluster to an externally managed Ceph cluster. While setting up Ceph as an OpenShift application is a relatively polished experienced, connecting to an external cluster still has some rough edges.
NB I am not a Ceph expert.</summary>
  </entry>
  
  <entry>
    <title>Getting started with KSOPS</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/"/>
    <id>https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/</id>
    <published>2021-03-09T00:00:00Z</published>
    <updated>2021-03-09T00:00:00Z</updated>
    <summary type="html">Kustomize is a tool for assembling Kubernetes manifests from a collection of files. We&amp;rsquo;re making extensive use of Kustomize in the operate-first project. In order to keep secrets stored in our configuration repositories, we&amp;rsquo;re using the KSOPS plugin, which enables Kustomize to use sops to encrypt/files using GPG.
In this post, I&amp;rsquo;d like to walk through the steps necessary to get everything up and running.
Set up GPG We encrypt files using GPG, so the first step is making sure that you have a GPG keypair and that your public key is published where other people can find it.</summary>
  </entry>
  
  <entry>
    <title>Object storage with OpenShift Container Storage</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/"/>
    <id>https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/</id>
    <published>2021-02-10T00:00:00Z</published>
    <updated>2021-02-10T00:00:00Z</updated>
    <summary type="html">OpenShift Container Storage (OCS) from Red Hat deploys Ceph in your OpenShift cluster (or allows you to integrate with an external Ceph cluster). In addition to the file- and block- based volume services provided by Ceph, OCS includes two S3-api compatible object storage implementations.
The first option is the Ceph Object Gateway (radosgw), Ceph&amp;rsquo;s native object storage interface. The second option called the &amp;ldquo;Multicloud Object Gateway&amp;rdquo;, which is in fact a piece of software named Noobaa, a storage abstraction layer that was acquired by Red Hat in 2018.</summary>
  </entry>
  
  <entry>
    <title>Heat-kubernetes Demo with Autoscaling</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/"/>
    <id>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</id>
    <published>2015-06-19T00:00:00Z</published>
    <updated>2015-06-19T00:00:00Z</updated>
    <summary type="html">Next week is the Red Hat Summit in Boston, and I&amp;rsquo;ll be taking part in a Project Atomic presentation in which I will discuss various (well, two) options for deploying Atomic into an OpenStack environment, focusing on my heat-kubernetes templates.
As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:
 This shows off the autoscaling behavior available with recent versions of these templates (and also serves as a very brief introduction to working with Kubernetes).</summary>
  </entry>
  
  <entry>
    <title>External networking for Kubernetes services</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/"/>
    <id>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</id>
    <published>2015-02-10T00:00:00Z</published>
    <updated>2015-02-10T00:00:00Z</updated>
    <summary type="html">I have recently started running some &amp;ldquo;real&amp;rdquo; services (that is, &amp;ldquo;services being consumed by someone other than myself&amp;rdquo;) on top of Kubernetes (running on bare metal), which means I suddenly had to confront the question of how to provide external access to Kubernetes hosted services. Kubernetes provides two solutions to this problem, neither of which is particularly attractive out of the box:
  There is a field createExternalLoadBalancer that can be set in a service description.</summary>
  </entry>
  
  <entry>
    <title>Building a minimal web server for testing Kubernetes</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server/"/>
    <id>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server/</id>
    <published>2015-01-04T00:00:00Z</published>
    <updated>2015-01-04T00:00:00Z</updated>
    <summary type="html">I have recently been doing some work with Kubernetes, and wanted to put together a minimal image with which I could test service and pod deployment. Size in this case was critical: I wanted something that would download quickly when initially deployed, because I am often setting up and tearing down Kubernetes as part of my testing (and some of my test environments have poor external bandwidth).
Building thttpd My go-to minimal webserver is thttpd.</summary>
  </entry>
  
  <entry>
    <title>Fedora Atomic, OpenStack, and Kubernetes (oh my)</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/"/>
    <id>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</id>
    <published>2014-11-24T00:00:00Z</published>
    <updated>2014-11-24T00:00:00Z</updated>
    <summary type="html">While experimenting with Fedora Atomic, I was looking for an elegant way to automatically deploy Atomic into an OpenStack environment and then automatically schedule some Docker containers on the Atomic host. This post describes my solution.
Like many other cloud-targeted distributions, Fedora Atomic runs cloud-init when the system boots. We can take advantage of this to configure the system at first boot by providing a user-data blob to Nova when we boot the instance.</summary>
  </entry>
  
  <entry>
    <title>Docker networking with dedicated network containers</title>
    <author>
      <name>Lars Kellogg-Stedman</name>
    </author>
    <link rel="alternate" href="https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/"/>
    <id>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</id>
    <published>2014-10-06T00:00:00Z</published>
    <updated>2014-10-06T00:00:00Z</updated>
    <summary type="html">The current version of Docker has a very limited set of networking options:
 bridge &amp;ndash; connect a container to the Docker bridge host &amp;ndash; run the container in the global network namespace container:xxx &amp;ndash; connect a container to the network namespace of another container none &amp;ndash; do not configure any networking  If you need something more than that, you can use a tool like pipework to provision additional network interfaces inside the container, but this leads to a synchronization problem: pipework can only be used after your container is running.</summary>
  </entry>
  
</feed>
